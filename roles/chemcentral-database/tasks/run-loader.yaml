---

- include_tasks: prep.yaml

# A kubernetes (or OpenShift) host and an API key must be set.
# We will either have authenticated against an OpenShift server
# or the AWX kubernetes credentials will have injected suitable
# values for the K8S_AUTH_HOST and K8S_AUTH_API_KEY environment variables.
# Either way the ansible variables 'k8s_auth_host' and
# 'k8s_auth_api_key' will have been set.
- name: Assert authentication
  assert:
    that:
    - k8s_auth_host|length > 0
    - k8s_auth_api_key|length > 0

# Run the loader....
# but first check the sanity of variables
# before checking that the namespace exists.

- name: Check control variables
  assert:
    that:
    - ccd_loader_class|string|length > 0
    - ccd_loader_class|string != 'SetMe'
    - ccd_bucket_path|string|length > 0
    - ccd_bucket_path|string != 'SetMe'
    - ccd_loader_file|string != 'SetMe' or ccd_loader_path|string != 'SetMe'

- name: Check multi-file loader control variables
  assert:
    that:
    - ccd_loader_path|string|length > 0
    - ccd_loader_pattern|string|length > 0
    - ccd_loader_pattern|string != 'SetMe'
  when: ccd_loader_path|string != 'SetMe'

# Deploy the loader

- name: Seplot the laoder
  block:

  # Namespace must exist

  - name: Get expected namespace
    k8s_info:
      kind: Namespace
      name: "{{ ccd_namespace }}"
    register: namespace_info

  - name: Assert namespace exists
    assert:
      that: namespace_info.resources|length > 0
      fail_msg: Namespace '{{ ccd_namespace }}' does not exist

  # Deploy the S3 secrets...

  - name: Check secrets
    k8s_info:
      kind: Secret
      api_version: v1
      namespace: "{{ ccd_namespace }}"
      name: s3-secrets
    register: s3_s_result

  - name: Set secret facts
    set_fact:
      aws_access_key_id_fact: "{{ aws_access_key_id }}"
      aws_secret_access_key_fact: "{{ aws_secret_access_key }}"
    when: s3_s_result.resources|length == 0

  - name: Set secret facts (pre-deployed secrets)
    set_fact:
      aws_access_key_id_fact: "{{ s3_s_result.resources[0].data.aws_access_key_id|b64decode }}"
      aws_secret_access_key_fact: "{{ s3_s_result.resources[0].data.aws_secret_access_key|b64decode }}"
    when: s3_s_result.resources|length == 1

  - name: Deploy secrets
    k8s:
      definition: "{{ lookup('template', 'secret-s3.yaml.j2') }}"
    when: s3_s_result.resources|length == 0

  # Deploy S3 config (if not AWS)...

  - name: Deploy non-AWS config
    k8s:
      definition: "{{ lookup('template', 'configmap-s3-config.yaml.j2') }}"
    when: ccd_s3_endpoint|string|length > 0

  # The loader volume (and storage class checks)

  - name: Get {{ ccd_database_loader_volume_storageclass }} StorageClass
    k8s_info:
      kind: StorageClass
      name: "{{ ccd_database_loader_volume_storageclass }}"
    register: sc_result
    when: ccd_database_loader_volume_storageclass != " "

  - name: Assert {{ ccd_database_loader_volume_storageclass }} StorageClass
    assert:
      that: sc_result.resources|length == 1
      fail_msg: The {{ ccd_database_loader_volume_storageclass }} StorageClass must be available on the cluster
    when: ccd_database_loader_volume_storageclass != " "

  - name: Create loader volume claim
    k8s:
      definition: "{{ lookup('template', 'pvc-loader.yaml.j2') }}"
      wait: yes
      wait_timeout: "{{ wait_timeout }}"

  # Best practice ... wait for the PVC to bind.
  # e.g. wait until resources[0].status.phase == Bound (initially Pending)

  - name: Wait for loader volume claim to bind
    k8s_info:
      kind: PersistentVolumeClaim
      name: loader
      namespace: "{{ ccd_namespace }}"
    register: loader_v_result
    until: >-
      loader_v_result.resources|length > 0
      and loader_v_result.resources[0].status is defined
      and loader_v_result.resources[0].status.phase == 'Bound'
    delay: 5
    retries: "{{ (bind_timeout|int / 5)|int }}"
    when: bind_timeout|int > 0

  - name: Remove existing loader
    k8s:
      state: absent
      definition: "{{ lookup('template', 'job-loader.yaml.j2') }}"
      wait: yes
      wait_timeout: "{{ wait_timeout }}"

  # Delete the Pods left behind that are Completed ('Succeeded').
  # Kubernetes *DOES NOT* remove these Job-based Pods automatically
  # (see https://kubernetes.io/docs/concepts/workloads/controllers/job/).
  # Instead completed Jobs need to be removed manually by the user.
  # The logic that follows list all Jobs (Pods)
  # that have Succeeded and then deletes them.

  - name: Get Succeeded Loader Pods
    k8s_info:
      kind: Pod
      namespace: "{{ ccd_namespace }}"
      label_selectors:
      - name=loader
      field_selectors:
      - status.phase=Succeeded
    register: pods_result

  - name: Delete Succeeded Loader Pods
    k8s:
      kind: Pod
      namespace: "{{ ccd_namespace }}"
      name: "{{ item.metadata.name }}"
      state: absent
    loop: "{{ pods_result.resources }}"
    when: pods_result.resources|length > 0

  - name: Run loader
    k8s:
      definition: "{{ lookup('template', 'job-loader.yaml.j2') }}"

  module_defaults:
    group/k8s:
      host: "{{ k8s_auth_host }}"
      api_key: "{{ k8s_auth_api_key }}"
